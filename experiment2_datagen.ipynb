{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3215156f-6c05-4b90-98dd-ba14899bef23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mrmprocs: process 1 not removed\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Distributed /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.6/Distributed/src/cluster.jl:1041\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8-element Vector{Int64}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the script in parallel\n",
    "using Distributed\n",
    "\n",
    "# Add processes\n",
    "rmprocs(workers()) # This will remove all worker processes\n",
    "addprocs(8) # Change this to the number of cores you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63063747-6eb3-43e7-8111-28392e672c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 3:\t\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/github/DifferentiableUserModels-JT/Project.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "environment at `~/github/DifferentiableUserModels-JT/Project.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 8:\t\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/github/DifferentiableUserModels-JT/Project.toml`\n",
      "      From worker 7:\t\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/github/DifferentiableUserModels-JT/Project.toml`\n",
      "      From worker 4:\t\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/github/DifferentiableUserModels-JT/Project.toml`\n",
      "      From worker 2:\t\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/github/DifferentiableUserModels-JT/Project.toml`\n",
      "      From worker 5:\t\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/github/DifferentiableUserModels-JT/Project.toml`\n",
      "      From worker 9:\t\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/github/DifferentiableUserModels-JT/Project.toml`\n",
      "      From worker 6:\t\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/github/DifferentiableUserModels-JT/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "@everywhere begin\n",
    "    using Pkg\n",
    "    Pkg.activate(\".\")\n",
    "    Pkg.instantiate()\n",
    "    #Pkg.status()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08f3afe3-8f12-4a51-894d-d4fcfde3ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere begin\n",
    "    using ArgParse\n",
    "    using BSON\n",
    "    using Distributions\n",
    "    using Flux\n",
    "    using Stheno\n",
    "    using Tracker\n",
    "    using Printf\n",
    "    using HDF5\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d71ffe6-c800-4fe7-9134-882ff1ce862f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mReplacing docs for `Main.NeuralProcesses.Categorical :: Union{}` in module `Main.NeuralProcesses`\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base.Docs docs/Docs.jl:240\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mReplacing docs for `Main.NeuralProcesses.Categorical :: Union{}` in module `Main.NeuralProcesses`\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base.Docs docs/Docs.jl:240\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mReplacing docs for `Main.NeuralProcesses.Categorical :: Union{}` in module `Main.NeuralProcesses`\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base.Docs docs/Docs.jl:240\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mReplacing docs for `Main.NeuralProcesses.Categorical :: Union{}` in module `Main.NeuralProcesses`\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base.Docs docs/Docs.jl:240\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mReplacing docs for `Main.NeuralProcesses.Categorical :: Union{}` in module `Main.NeuralProcesses`\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base.Docs docs/Docs.jl:240\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mReplacing docs for `Main.NeuralProcesses.Categorical :: Union{}` in module `Main.NeuralProcesses`\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base.Docs docs/Docs.jl:240\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mReplacing docs for `Main.NeuralProcesses.Categorical :: Union{}` in module `Main.NeuralProcesses`\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base.Docs docs/Docs.jl:240\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mReplacing docs for `Main.NeuralProcesses.Categorical :: Union{}` in module `Main.NeuralProcesses`\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base.Docs docs/Docs.jl:240\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mReplacing docs for `Main.NeuralProcesses.Categorical :: Union{}` in module `Main.NeuralProcesses`\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base.Docs docs/Docs.jl:240\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@everywhere include(joinpath(@__DIR__, \"NeuralProcesses.jl/src/NeuralProcesses.jl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d340148-c756-4ce6-aa60-a10856516757",
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere using .NeuralProcesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d84537e7-d05d-4a82-8ce4-49e70b353218",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@everywhere begin\n",
    "    #include(joinpath(@__DIR__, \"NeuralProcesses.jl/src/NeuralProcesses.jl\"))\n",
    "    #include(\"NeuralProcesses.jl/src/NeuralProcesses.jl\")    \n",
    "    #using .NeuralProcesses\n",
    "#end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f3251e1-f348-4263-97d6-a73223f1b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# parser = ArgParseSettings()\n",
    "# @add_arg_table! parser begin\n",
    "#     \"--gen\"\n",
    "#         help = \"Experiment setting: gridworld, menu_search, h_menu_search\"\n",
    "#         arg_type = String\n",
    "#         default = \"menu_search\"\n",
    "#     \"--n_traj\"\n",
    "#         help = \"Number of context trajectories. Setting to 0 randomizes between 1 and 8.\"\n",
    "#         arg_type = Int\n",
    "#         default = 0\n",
    "#     \"--n_epochs\"\n",
    "#         help = \"Number of training epochs.\"\n",
    "#         arg_type = Int\n",
    "#         default = 50\n",
    "#     \"--n_batches\"\n",
    "#         help = \"Number of batches.\"\n",
    "#         arg_type = Int\n",
    "#         default = 25\n",
    "#     \"--batch_size\"\n",
    "#         help = \"Batch size.\"\n",
    "#         arg_type = Int\n",
    "#         default = 4\n",
    "#     \"--params\"\n",
    "#         help = \"Return params?\"\n",
    "#         arg_type = Bool\n",
    "#         default = false\n",
    "#     \"--p_bias\"\n",
    "#         help = \"Probability of generating a sample with biased model\"\n",
    "#         arg_type = Float64\n",
    "#         default = 0.0\n",
    "#     \"--bson\"\n",
    "#         help = \"Directly specify the file to save the model to and load it from.\"\n",
    "#         arg_type = String\n",
    "#     \"--epsilon\"\n",
    "#         help = \"Value for epsilon.\"\n",
    "#         arg_type = Float64\n",
    "# end\n",
    "# args = parse_args(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4855248-26f0-41dd-ad0e-3ecad93967c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary to just use the default arguments from the argument parser\n",
    "@everywhere begin\n",
    "    function get_default_args()\n",
    "        defaults = Dict(\n",
    "            \"gen\" => \"menu_search\",\n",
    "            \"n_traj\" => 0,\n",
    "            \"n_epochs\" => 50,\n",
    "            \"n_batches\" => 25,\n",
    "            \"batch_size\" => 4,\n",
    "            \"params\" => false,\n",
    "            \"p_bias\" => 0.0,\n",
    "            \"bson\" => \"\",\n",
    "            \"epsilon\" => 0.0\n",
    "        )\n",
    "        return defaults\n",
    "    end\n",
    "    \n",
    "    args = get_default_args()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e459a34-1815-4129-9068-c84d78ca0da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't bother initializing the model\n",
    "# println(\"Initializing model...\")\n",
    "\n",
    "# model = anp_ex2(\n",
    "#     dim_embedding=128,\n",
    "#     num_encoder_heads=8,\n",
    "#     num_encoder_layers=6,\n",
    "#     num_decoder_layers=6,\n",
    "#     args=args\n",
    "# ) |> gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94735e4c-93cc-4ce1-bb2d-830288291217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't bother initializing the loss\n",
    "# println(\"Initializing loss...\")\n",
    "\n",
    "# loss(xs...) = np_elbo(\n",
    "#     xs...,\n",
    "#     num_samples=5,\n",
    "#     fixed_σ_epochs=3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7ecbf38-829d-4cc3-ad23-5f97e0b7c579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data generator\n",
      "Data gen initialized\n",
      "      From worker 2:\tInitializing data generator\n",
      "      From worker 5:\tInitializing data generator\n",
      "      From worker 4:\tInitializing data generator\n",
      "      From worker 8:\tInitializing data generator\n",
      "      From worker 3:\tInitializing data generator\n",
      "      From worker 6:\tInitializing data generator\n",
      "      From worker 7:\tInitializing data generator\n",
      "      From worker 9:\tInitializing data generator\n",
      "      From worker 4:\tData gen initialized\n",
      "      From worker 6:\tData gen initialized\n",
      "      From worker 2:\tData gen initialized\n",
      "      From worker 5:\tData gen initialized\n",
      "      From worker 7:\tData gen initialized\n",
      "      From worker 3:\tData gen initialized\n",
      "      From worker 9:\tData gen initialized\n",
      "      From worker 8:\tData gen initialized\n"
     ]
    }
   ],
   "source": [
    "# Make the data generator\n",
    "@everywhere begin\n",
    "    println(\"Initializing data generator\")\n",
    "    \n",
    "    batch_size  = args[\"batch_size\"]\n",
    "    \n",
    "    # Redundant. Required to fit the DataGenerator definition\n",
    "    x_context = Distributions.Uniform(-2, 2)\n",
    "    x_target  = Distributions.Uniform(-2, 2)\n",
    "    \n",
    "    num_context = Distributions.DiscreteUniform(10, 10)\n",
    "    num_target  = Distributions.DiscreteUniform(10, 10)\n",
    "    \n",
    "    data_gen = NeuralProcesses.DataGenerator(\n",
    "                    SearchEnvSampler(args;),\n",
    "                    batch_size=batch_size,\n",
    "                    x_context=x_context,\n",
    "                    x_target=x_target,\n",
    "                    num_context=num_context,\n",
    "                    num_target=num_target,\n",
    "                    σ²=1e-8\n",
    "                )\n",
    "    println(\"Data gen initialized\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a541d13-6eed-464d-9e16-01725b9aff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch:       0     \n",
      "Tasks per epoch:      32    \n",
      "      From worker 9:\tStarting epoch:       0     Batch size:           4     \n",
      "\n",
      "Number of batches     25    \n",
      "      From worker 9:\tTasks per epoch:      32    \n",
      "      From worker 9:\tBatch size:           4     \n",
      "      From worker 9:\tNumber of batches     25    \n",
      "      From worker 8:\tStarting epoch:       0     \n",
      "      From worker 8:\tTasks per epoch:      32    \n"
     ]
    }
   ],
   "source": [
    "@everywhere begin\n",
    "    # Variables normally defined in the part where you train the model\n",
    "    tasks_per_epoch=2^5\n",
    "    #total_epochs = total_epochs=args[\"n_epochs\"]\n",
    "    starting_epoch=0\n",
    "    batches=args[\"n_batches\"]\n",
    "    experiment = \"menu_search\"\n",
    "    \n",
    "    # Divide out batch size to get the number of batches per epoch.\n",
    "    batches_per_epoch = div(tasks_per_epoch, data_gen.batch_size)\n",
    "    \n",
    "    # Display the settings of the training run.\n",
    "    #@printf(\"Epochs:               %-6d\\n\", total_epochs)\n",
    "    @printf(\"Starting epoch:       %-6d\\n\", starting_epoch)\n",
    "    @printf(\"Tasks per epoch:      %-6d\\n\", batches_per_epoch * data_gen.batch_size)\n",
    "    @printf(\"Batch size:           %-6d\\n\", data_gen.batch_size)\n",
    "    @printf(\"Number of batches     %-6d\\n\", batches)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a08e0bd2-89b3-4885-9ead-bf30ae4b47a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 8:\tBatch size:           4     \n",
      "      From worker 8:\tNumber of batches     25    \n",
      "      From worker 3:\tStarting epoch:       0     \n",
      "      From worker 3:\tTasks per epoch:      32    \n",
      "      From worker 3:\tBatch size:           4     \n"
     ]
    }
   ],
   "source": [
    "# # Use the data generator\n",
    "# for batch_n in 1:batches-1\n",
    "#     # Warmup epoch\n",
    "#     if batch_n == starting_epoch\n",
    "#         n_mini_batches = 1\n",
    "#     else\n",
    "#         n_mini_batches = batches_per_epoch\n",
    "#     end\n",
    "#     # Generate data\n",
    "#     data = gen_batch(data_gen, n_mini_batches; eval=false)\n",
    "\n",
    "#     if experiment == \"menu_search\"\n",
    "#         BSON.bson(\"data/ex2/\"*string(batch_n)*\".bson\", data=data)\n",
    "#     end\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0c1894f-365c-433e-89fd-a07fe7d2065b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 3:\tNumber of batches     25    \n",
      "      From worker 2:\tStarting epoch:       0     \n",
      "      From worker 2:\tTasks per epoch:      32    \n",
      "      From worker 2:\tBatch size:           4     \n",
      "      From worker 2:\tNumber of batches     25    \n",
      "      From worker 5:\tStarting epoch:       0     \n",
      "      From worker 5:\tTasks per epoch:      32    \n",
      "      From worker 5:\tBatch size:           4     \n",
      "      From worker 5:\tNumber of batches     25    \n",
      "      From worker 6:\tStarting epoch:       0     \n",
      "      From worker 6:\tTasks per epoch:      32    \n",
      "      From worker 6:\tBatch size:           4     \n",
      "      From worker 6:\tNumber of batches     25    \n",
      "      From worker 4:\tStarting epoch:       0     \n",
      "      From worker 4:\tTasks per epoch:      32    \n",
      "      From worker 4:\tBatch size:           4     \n",
      "      From worker 4:\tNumber of batches     25    \n",
      "      From worker 7:\tStarting epoch:       0     \n",
      "      From worker 7:\tTasks per epoch:      32    \n",
      "      From worker 7:\tBatch size:           4     \n",
      "      From worker 7:\tNumber of batches     25    \n"
     ]
    }
   ],
   "source": [
    "@everywhere begin\n",
    "    # Add multiple pieces of metadata to the dataset\n",
    "    \n",
    "    metadata = Dict(\n",
    "    \"gen_type\" => \"SearchEnvSampler / menu_search\",\n",
    "    \"eval\" => false,\n",
    "    \"batch_size\" => batch_size,\n",
    "    \"n_traj\" => \"random(1-8)\", #This is what happens when it's set to 0 in args dictionary\n",
    "    \"noise_variance\" => 1e-8,\n",
    "    \"p_bias\" => args[\"p_bias\"],\n",
    "    \"epsilon\" => args[\"epsilon\"]\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    function create_hdf5_ex2(data, filename, metadata)\n",
    "        # Open the HDF5 file for writing, overwriting if it exists\n",
    "        h5open(filename, \"w\") do fid\n",
    "            # Loop over the data vector\n",
    "            for (i, d) in enumerate(data)\n",
    "                # Create a group for each mini-batch\n",
    "                g = create_group(fid, \"mini_batch_$i\")\n",
    "    \n",
    "                # Add datasets to the group\n",
    "                g[\"xc\"] = d[1]\n",
    "                g[\"yc\"] = d[2]\n",
    "                g[\"xt\"] = d[3]\n",
    "                g[\"yt\"] = d[4]\n",
    "    \n",
    "                # Add metadata to the group\n",
    "                for (key, value) in metadata\n",
    "                    write_attribute(g, key, value)\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb3e28f6-fd5d-4ed4-bb18-2e755a99f228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Task (runnable) @0x000000019e09a650"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@distributed for batch_n in 1:batches-1\n",
    "#for batch_n in 1:batches-1\n",
    "    @printf(\"Running batch:               %-6d\\n\", batch_n)\n",
    "    # Warmup epoch\n",
    "    if batch_n == starting_epoch\n",
    "        n_mini_batches = 1\n",
    "    else\n",
    "        n_mini_batches = batches_per_epoch\n",
    "    end\n",
    "\n",
    "    # Add minibatches to metadata\n",
    "    metadata[\"n_mini_batches\"] = n_mini_batches\n",
    "    \n",
    "    # Generate data\n",
    "    @printf(\"Generating batch:               %-6d\\n\", batch_n)\n",
    "    data = gen_batch(data_gen, n_mini_batches; eval=false)\n",
    "    \n",
    "    @printf(\"Saving batch:               %-6d\\n\", batch_n)\n",
    "    filename = \"data/ex2/experiment2_batch\"*string(batch_n)*\".hdf\"\n",
    "    create_hdf5_ex2(data, filename, metadata)\n",
    "    @printf(\"Finished batch:               %-6d\\n\", batch_n)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6c4ab61-4851-4579-a98a-71fec00a4d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 5:\tRunning batch:               10    \n",
      "      From worker 5:\tGenerating batch:               10    \n",
      "      From worker 6:\tRunning batch:               13    \n",
      "      From worker 6:\tGenerating batch:               13    \n",
      "      From worker 4:\tRunning batch:               7     \n",
      "      From worker 4:\tGenerating batch:               7     \n",
      "      From worker 8:\tRunning batch:               19    \n",
      "      From worker 8:\tGenerating batch:               19    \n",
      "      From worker 2:\tRunning batch:               1     \n",
      "      From worker 2:\tGenerating batch:               1     \n",
      "      From worker 3:\tRunning batch:               4     \n",
      "      From worker 3:\tGenerating batch:               4     \n",
      "      From worker 7:\tRunning batch:               16    \n",
      "      From worker 7:\tGenerating batch:               16    \n",
      "      From worker 9:\tRunning batch:               22    \n",
      "      From worker 9:\tGenerating batch:               22    \n",
      "      From worker 5:\tSaving batch:               10    \n",
      "      From worker 5:\tFinished batch:               10    \n",
      "      From worker 5:\tRunning batch:               11    \n",
      "      From worker 5:\tGenerating batch:               11    \n",
      "      From worker 7:\tSaving batch:               16    \n",
      "      From worker 7:\tFinished batch:               16    \n",
      "      From worker 7:\tRunning batch:               17    \n",
      "      From worker 7:\tGenerating batch:               17    \n",
      "      From worker 2:\tSaving batch:               1     \n",
      "      From worker 4:\tSaving batch:               7     \n",
      "      From worker 2:\tFinished batch:               1     \n",
      "      From worker 2:\tRunning batch:               2     \n",
      "      From worker 2:\tGenerating batch:               2     \n",
      "      From worker 4:\tFinished batch:               7     \n",
      "      From worker 4:\tRunning batch:               8     \n",
      "      From worker 4:\tGenerating batch:               8     \n",
      "      From worker 6:\tSaving batch:               13    \n",
      "      From worker 6:\tFinished batch:               13    \n",
      "      From worker 6:\tRunning batch:               14    \n",
      "      From worker 6:\tGenerating batch:               14    \n",
      "      From worker 9:\tSaving batch:               22    \n",
      "      From worker 9:\tFinished batch:               22    \n",
      "      From worker 9:\tRunning batch:               23    \n",
      "      From worker 9:\tGenerating batch:               23    \n",
      "      From worker 3:\tSaving batch:               4     \n",
      "      From worker 8:\tSaving batch:               19    \n",
      "      From worker 3:\tFinished batch:               4     \n",
      "      From worker 3:\tRunning batch:               5     \n",
      "      From worker 3:\tGenerating batch:               5     \n",
      "      From worker 8:\tFinished batch:               19    \n",
      "      From worker 8:\tRunning batch:               20    \n",
      "      From worker 8:\tGenerating batch:               20    \n",
      "      From worker 2:\tSaving batch:               2     \n",
      "      From worker 2:\tFinished batch:               2     \n",
      "      From worker 2:\tRunning batch:               3     \n",
      "      From worker 2:\tGenerating batch:               3     \n",
      "      From worker 5:\tSaving batch:               11    \n",
      "      From worker 5:\tFinished batch:               11    \n",
      "      From worker 5:\tRunning batch:               12    \n",
      "      From worker 5:\tGenerating batch:               12    \n",
      "      From worker 4:\tSaving batch:               8     \n",
      "      From worker 4:\tFinished batch:               8     \n",
      "      From worker 4:\tRunning batch:               9     \n",
      "      From worker 4:\tGenerating batch:               9     \n",
      "      From worker 7:\tSaving batch:               17    \n",
      "      From worker 7:\tFinished batch:               17    \n",
      "      From worker 7:\tRunning batch:               18    \n",
      "      From worker 7:\tGenerating batch:               18    \n",
      "      From worker 6:\tSaving batch:               14    \n",
      "      From worker 6:\tFinished batch:               14    \n",
      "      From worker 6:\tRunning batch:               15    \n",
      "      From worker 6:\tGenerating batch:               15    \n",
      "      From worker 3:\tSaving batch:               5     \n",
      "      From worker 3:\tFinished batch:               5     \n",
      "      From worker 3:\tRunning batch:               6     \n",
      "      From worker 3:\tGenerating batch:               6     \n",
      "      From worker 9:\tSaving batch:               23    \n",
      "      From worker 9:\tFinished batch:               23    \n",
      "      From worker 9:\tRunning batch:               24    \n",
      "      From worker 9:\tGenerating batch:               24    \n",
      "      From worker 8:\tSaving batch:               20    \n",
      "      From worker 8:\tFinished batch:               20    \n",
      "      From worker 8:\tRunning batch:               21    \n",
      "      From worker 8:\tGenerating batch:               21    \n",
      "      From worker 4:\tSaving batch:               9     \n",
      "      From worker 4:\tFinished batch:               9     \n",
      "      From worker 2:\tSaving batch:               3     \n",
      "      From worker 2:\tFinished batch:               3     \n",
      "      From worker 5:\tSaving batch:               12    \n",
      "      From worker 5:\tFinished batch:               12    \n",
      "      From worker 7:\tSaving batch:               18    \n",
      "      From worker 7:\tFinished batch:               18    \n",
      "      From worker 6:\tSaving batch:               15    \n",
      "      From worker 6:\tFinished batch:               15    \n",
      "      From worker 3:\tSaving batch:               6     \n",
      "      From worker 3:\tFinished batch:               6     \n",
      "      From worker 9:\tSaving batch:               24    \n",
      "      From worker 9:\tFinished batch:               24    \n",
      "      From worker 8:\tSaving batch:               21    \n",
      "      From worker 8:\tFinished batch:               21    \n"
     ]
    }
   ],
   "source": [
    "# Don't bother training the model\n",
    "# println(\"Proceeding to training loop...\")\n",
    "\n",
    "# mkpath(\"models/\"*string(args[\"bson\"]))\n",
    "\n",
    "# train_model!(\n",
    "#         model,\n",
    "#         loss,\n",
    "#         data_gen,\n",
    "#         ADAM(5e-4),\n",
    "#         bson=args[\"bson\"],\n",
    "# \texperiment=args[\"gen\"],\n",
    "#         starting_epoch=0,\n",
    "#         tasks_per_epoch=2^5,\n",
    "#         batches=args[\"n_batches\"],\n",
    "# \ttotal_epochs=args[\"n_epochs\"],\n",
    "#         epsilon=args[\"epsilon\"]\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.7",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
