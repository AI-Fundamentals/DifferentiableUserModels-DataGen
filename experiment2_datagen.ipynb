{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f959899-19ce-4892-b6b5-933a2d85f526",
   "metadata": {},
   "source": [
    "## Script to generate data for experiment2.jl\n",
    "The original script generated was quite confused (and confusing) in terms of the number of the number of tasks/batches/minibatches. Originally the script created 192 tasks split up into 24 batches. This calculation comes from an input of \"2^5 (32) tasks\" multiplied by 25 (actually 24) batches, where a batch is an arbitrary way to split the training dataset into separate files. In this implementation, the training dataset is simply split into one HDF5 file of 192 tasks, so the minibatches can be dealt with when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3215156f-6c05-4b90-98dd-ba14899bef23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mrmprocs: process 1 not removed\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Distributed /Users/julia/buildbot/worker/package_macos64/build/usr/share/julia/stdlib/v1.6/Distributed/src/cluster.jl:1041\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8-element Vector{Int64}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the script in parallel\n",
    "using Distributed\n",
    "\n",
    "# Add processes\n",
    "rmprocs(workers()) # This will remove all worker processes\n",
    "n_workers = 8\n",
    "addprocs(n_workers) # Change this to the number of cores you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63063747-6eb3-43e7-8111-28392e672c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/github/DifferentiableUserModels-JT/Project.toml`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      From worker 7:\t\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/github/DifferentiableUserModels-JT/Project.toml`\n",
      "      From worker 8:\t\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/github/DifferentiableUserModels-JT/Project.toml`\n",
      "      From worker 4:\t\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/github/DifferentiableUserModels-JT/Project.toml`\n",
      "      From worker 9:\t\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/github/DifferentiableUserModels-JT/Project.toml`\n",
      "      From worker 3:\t\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/github/DifferentiableUserModels-JT/Project.toml`\n",
      "      From worker 2:\t\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/github/DifferentiableUserModels-JT/Project.toml`\n",
      "      From worker 5:\t\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m environment at `~/github/DifferentiableUserModels-JT/Project.toml`\n"
     ]
    }
   ],
   "source": [
    "@everywhere begin\n",
    "    using Pkg\n",
    "    Pkg.activate(\".\")\n",
    "    Pkg.instantiate()\n",
    "    #Pkg.status()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08f3afe3-8f12-4a51-894d-d4fcfde3ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere begin\n",
    "    using ArgParse\n",
    "    using BSON\n",
    "    using Distributions\n",
    "    using Flux\n",
    "    using Stheno\n",
    "    using Tracker\n",
    "    using Printf\n",
    "    using HDF5\n",
    "    using SharedArrays\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d71ffe6-c800-4fe7-9134-882ff1ce862f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mReplacing docs for `Main.NeuralProcesses.Categorical :: Union{}` in module `Main.NeuralProcesses`\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base.Docs docs/Docs.jl:240\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mReplacing docs for `Main.NeuralProcesses.Categorical :: Union{}` in module `Main.NeuralProcesses`\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base.Docs docs/Docs.jl:240\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mReplacing docs for `Main.NeuralProcesses.Categorical :: Union{}` in module `Main.NeuralProcesses`\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base.Docs docs/Docs.jl:240\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mReplacing docs for `Main.NeuralProcesses.Categorical :: Union{}` in module `Main.NeuralProcesses`\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base.Docs docs/Docs.jl:240\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mReplacing docs for `Main.NeuralProcesses.Categorical :: Union{}` in module `Main.NeuralProcesses`\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base.Docs docs/Docs.jl:240\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mReplacing docs for `Main.NeuralProcesses.Categorical :: Union{}` in module `Main.NeuralProcesses`\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base.Docs docs/Docs.jl:240\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mReplacing docs for `Main.NeuralProcesses.Categorical :: Union{}` in module `Main.NeuralProcesses`\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base.Docs docs/Docs.jl:240\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mReplacing docs for `Main.NeuralProcesses.Categorical :: Union{}` in module `Main.NeuralProcesses`\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base.Docs docs/Docs.jl:240\u001b[39m\n",
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mReplacing docs for `Main.NeuralProcesses.Categorical :: Union{}` in module `Main.NeuralProcesses`\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Base.Docs docs/Docs.jl:240\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "@everywhere include(joinpath(@__DIR__, \"NeuralProcesses.jl/src/NeuralProcesses.jl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d340148-c756-4ce6-aa60-a10856516757",
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere using .NeuralProcesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4855248-26f0-41dd-ad0e-3ecad93967c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary to just use the default arguments from the argument parser\n",
    "# Not all of these are used in this script\n",
    "@everywhere begin\n",
    "    function get_default_args()\n",
    "        defaults = Dict(\n",
    "            \"gen\" => \"menu_search\",\n",
    "            \"n_traj\" => 0,\n",
    "            \"n_epochs\" => 50,\n",
    "            \"n_batches\" => 25,\n",
    "            \"batch_size\" => 4,\n",
    "            \"params\" => false,\n",
    "            \"p_bias\" => 0.0,\n",
    "            \"bson\" => \"\",\n",
    "            \"epsilon\" => 0.0\n",
    "        )\n",
    "        return defaults\n",
    "    end\n",
    "    \n",
    "    args = get_default_args()\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e459a34-1815-4129-9068-c84d78ca0da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't bother initializing the model\n",
    "# println(\"Initializing model...\")\n",
    "\n",
    "# model = anp_ex2(\n",
    "#     dim_embedding=128,\n",
    "#     num_encoder_heads=8,\n",
    "#     num_encoder_layers=6,\n",
    "#     num_decoder_layers=6,\n",
    "#     args=args\n",
    "# ) |> gpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94735e4c-93cc-4ce1-bb2d-830288291217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't bother initializing the loss\n",
    "# println(\"Initializing loss...\")\n",
    "\n",
    "# loss(xs...) = np_elbo(\n",
    "#     xs...,\n",
    "#     num_samples=5,\n",
    "#     fixed_σ_epochs=3\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7ecbf38-829d-4cc3-ad23-5f97e0b7c579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data generator\n",
      "Data gen initialized\n",
      "      From worker 4:\tInitializing data generator\n",
      "      From worker 4:\tData gen initialized\n",
      "      From worker 3:\tInitializing data generator\n",
      "      From worker 9:\tInitializing data generator\n",
      "      From worker 9:\tData gen initialized\n",
      "      From worker 5:\tInitializing data generator\n",
      "      From worker 7:\tInitializing data generator\n",
      "      From worker 8:\tInitializing data generator\n",
      "      From worker 6:\tInitializing data generator\n",
      "      From worker 2:\tInitializing data generator\n",
      "      From worker 5:\tData gen initialized\n",
      "      From worker 6:\tData gen initialized\n",
      "      From worker 8:\tData gen initialized\n",
      "      From worker 3:\tData gen initialized\n",
      "      From worker 7:\tData gen initialized\n",
      "      From worker 2:\tData gen initialized\n"
     ]
    }
   ],
   "source": [
    "# Make the data generator\n",
    "@everywhere begin\n",
    "    println(\"Initializing data generator\")\n",
    "    \n",
    "    batch_size  = args[\"batch_size\"]\n",
    "    \n",
    "    # Redundant. Required to fit the DataGenerator definition\n",
    "    x_context = Distributions.Uniform(-2, 2)\n",
    "    x_target  = Distributions.Uniform(-2, 2)\n",
    "    \n",
    "    num_context = Distributions.DiscreteUniform(10, 10)\n",
    "    num_target  = Distributions.DiscreteUniform(10, 10)\n",
    "    \n",
    "    data_gen = NeuralProcesses.DataGenerator(\n",
    "                    SearchEnvSampler(args;),\n",
    "                    batch_size=batch_size,\n",
    "                    x_context=x_context,\n",
    "                    x_target=x_target,\n",
    "                    num_context=num_context,\n",
    "                    num_target=num_target,\n",
    "                    σ²=1e-8\n",
    "                )\n",
    "    println(\"Data gen initialized\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cb3e28f6-fd5d-4ed4-bb18-2e755a99f228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting generating data with 8 workers\n",
      "      From worker 5:\tStarting task 7\n",
      "      From worker 3:\tStarting task 3\n",
      "      From worker 2:\tStarting task 1\n",
      "      From worker 9:\tStarting task 15\n",
      "      From worker 8:\tStarting task 13\n",
      "      From worker 7:\tStarting task 11\n",
      "      From worker 6:\tStarting task 9\n",
      "      From worker 4:\tStarting task 5\n",
      "      From worker 2:\tStarting task 2\n",
      "      From worker 9:\tStarting task 16\n",
      "      From worker 5:\tStarting task 8\n",
      "      From worker 7:\tStarting task 12\n",
      "      From worker 4:\tStarting task 6\n",
      "      From worker 3:\tStarting task 4\n",
      "      From worker 8:\tStarting task 14\n",
      "      From worker 6:\tStarting task 10\n",
      "Finished generating data\n"
     ]
    }
   ],
   "source": [
    "# Generate the data in a parallel way. The vector \"data\" will be the dataset from\n",
    "# all 192 tasks\n",
    "\n",
    "tasks_per_epoch = 16\n",
    "\n",
    "# Function to help print output in realtime in jupyter notebooks\n",
    "\n",
    "println(\"Starting generating data with $n_workers workers\")\n",
    "data = @distributed (vcat) for task_n in 1:tasks_per_epoch;\n",
    "    println(\"Starting task $task_n\")\n",
    "    flush(stdout)\n",
    "    \n",
    "    # Generate data\n",
    "    data = gen_batch(data_gen, 1; eval=false)\n",
    "    #data = gen_batch(data_gen, tasks_per_worker; eval=false)\n",
    "    \n",
    "    # Return the data from this worker to the \"big\" data array above\n",
    "    data;\n",
    "end\n",
    "\n",
    "println(\"Finished generating data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d0c1894f-365c-433e-89fd-a07fe7d2065b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_hdf5_ex2 (generic function with 1 method)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add multiple pieces of metadata to the dataset   \n",
    "metadata = Dict(\n",
    "\"gen_type\" => \"SearchEnvSampler / menu_search\",\n",
    "\"tasks_per_epoch\" => tasks_per_epoch,\n",
    "\"eval\" => false,\n",
    "\"batch_size\" => batch_size,\n",
    "\"n_traj\" => \"random(1-8), although this doesn't seem to be used\", #This is what happens when it's set to 0 in args dictionary\n",
    "\"noise_variance\" => 1e-8,\n",
    "\"p_bias\" => 0.0\n",
    ")\n",
    "\n",
    "# Function to save the data as HDF5\n",
    "function create_hdf5_ex2(data, filename, metadata)\n",
    "    # Open the HDF5 file for writing, overwriting if it exists\n",
    "    h5open(filename, \"w\") do fid\n",
    "        # Loop over the data vector\n",
    "        for (i, d) in enumerate(data)\n",
    "            # Create a group for each mini-batch\n",
    "            g = create_group(fid, \"task_$i\")\n",
    "\n",
    "            # Add datasets to the group\n",
    "            g[\"xc\"] = d[1]\n",
    "            g[\"yc\"] = d[2]\n",
    "            g[\"xt\"] = d[3]\n",
    "            g[\"yt\"] = d[4]\n",
    "\n",
    "            # Add metadata to the group\n",
    "            for (key, value) in metadata\n",
    "                write_attribute(g, key, value)\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec352ec9-d70f-46a0-979b-bede2f168e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved successfully\n"
     ]
    }
   ],
   "source": [
    "# Save the data!\n",
    "\n",
    "filepath = \"data/ex2/experiment2_data.hdf\"\n",
    "create_hdf5_ex2(data,filepath,metadata)\n",
    "println(\"File saved successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c4ab61-4851-4579-a98a-71fec00a4d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't bother training the model\n",
    "# println(\"Proceeding to training loop...\")\n",
    "\n",
    "# mkpath(\"models/\"*string(args[\"bson\"]))\n",
    "\n",
    "# train_model!(\n",
    "#         model,\n",
    "#         loss,\n",
    "#         data_gen,\n",
    "#         ADAM(5e-4),\n",
    "#         bson=args[\"bson\"],\n",
    "# \texperiment=args[\"gen\"],\n",
    "#         starting_epoch=0,\n",
    "#         tasks_per_epoch=2^5,\n",
    "#         batches=args[\"n_batches\"],\n",
    "# \ttotal_epochs=args[\"n_epochs\"],\n",
    "#         epsilon=args[\"epsilon\"]\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.7",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
